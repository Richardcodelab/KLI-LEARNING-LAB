# -*- coding: utf-8 -*-
"""
Streamlit í†µí•© ì•± (MVP)
- ê²€ìƒ‰ì–´ ì •ê·œí™” (CSV + AI ì˜µì…˜)
- KCI & RISS ê²€ìƒ‰ ì‹¤í–‰
- ê²°ê³¼ ë³‘í•©/ì¤‘ë³µì œê±°/í•„í„°ë§/ë‹¤ìš´ë¡œë“œ
- ê¸°ë³¸ ë¶„ì„ íƒ­ (ì—°ë„/ì†ŒìŠ¤/í•™ìˆ ì§€)

í•„ìˆ˜ íŒŒì¼: query_normalizer.py, KCI_collector.py, RISS_collector.py, keyword_mapping.csv, .env
ì‹¤í–‰: streamlit run app.py
"""

import os
import io
import re
import time
from datetime import datetime
from typing import List, Optional

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import streamlit as st
from dotenv import load_dotenv

# ë‚´ë¶€ ëª¨ë“ˆ
from query_normalizer import QueryNormalizer
from KCI_collector import OptimizedKCIAnalyzer
from RISS_collector import ImprovedRISSAnalyzer

# --- ì´ˆê¸° ì„¤ì • ---
st.set_page_config(page_title="ëŸ¬ë‹ë© ì„ í–‰ì—°êµ¬ ìë™ë¶„ë¥˜ê¸°", layout="wide")
load_dotenv()  # .env ë¡œë“œ

KCI_API_KEY = os.getenv("KCI_API_KEY", "")
RISS_API_KEY = os.getenv("RISS_API_KEY", "")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")

# --- ìœ í‹¸ ---

def normalize_title(t: str) -> str:
    if not isinstance(t, str):
        return ""
    # ê³µë°±/íŠ¹ìˆ˜ë¬¸ì ì œê±°, ì†Œë¬¸ìí™”
    t = re.sub(r"\s+", " ", t).strip().lower()
    t = re.sub(r"[^0-9a-zê°€-í£ ]", "", t)
    return t


def to_excel_bytes(df: pd.DataFrame, sheets: Optional[dict] = None) -> bytes:
    """DataFrame ë˜ëŠ” ë³µìˆ˜ ì‹œíŠ¸ ì—‘ì…€ ë°”ì´íŠ¸ ìƒì„±"""
    data = io.BytesIO()
    with pd.ExcelWriter(data, engine="openpyxl") as writer:
        if sheets:
            for name, d in sheets.items():
                d.to_excel(writer, sheet_name=name[:31], index=False)
        else:
            df.to_excel(writer, sheet_name="results", index=False)
    data.seek(0)
    return data.read()


def merge_frames(df_kci: Optional[pd.DataFrame], df_riss: Optional[pd.DataFrame]) -> pd.DataFrame:
    frames = []
    # KCI í‘œì¤€í™”
    if df_kci is not None and not df_kci.empty:
        kci = df_kci.copy()
        kci["source"] = "KCI"
        # í‘œì¤€ ìŠ¤í‚¤ë§ˆ ë§¤í•‘
        kci.rename(
            columns={
                "journal_name": "venue",
                "authors": "authors",
                "pub_year": "pub_year",
                "title": "title",
                "url": "url",
                "doi": "doi",
                "abstract": "abstract",
                "keywords": "keywords",
            },
            inplace=True,
        )
        # ëˆ„ë½ ì»¬ëŸ¼ ë³´ê°•
        for col in ["authors", "venue", "pub_year", "url", "doi", "abstract", "keywords"]:
            if col not in kci.columns:
                kci[col] = ""
        frames.append(kci[[
            "title", "authors", "venue", "pub_year", "url", "doi", "abstract", "keywords", "source"
        ]])

    # RISS í‘œì¤€í™”
    if df_riss is not None and not df_riss.empty:
        riss = df_riss.copy()
        riss["source"] = "RISS"
        # RISS_collector ìŠ¤í‚¤ë§ˆ ì°¸ê³ 
        # title, author, publisher, pub_year, doc_type, material_type, url, has_abstract ...
        riss.rename(
            columns={
                "author": "authors",
                "publisher": "venue",
            },
            inplace=True,
        )
        # ë³´ê°• ì»¬ëŸ¼
        for col in ["doi", "abstract", "keywords"]:
            if col not in riss.columns:
                riss[col] = ""
        # í‘œì¤€ ì»¬ëŸ¼ë§Œ ì •ë ¬
        keep = ["title", "authors", "venue", "pub_year", "url", "doi", "abstract", "keywords", "source", "doc_type", "material_type"]
        for c in keep:
            if c not in riss.columns:
                riss[c] = ""
        frames.append(riss[keep])

    if not frames:
        return pd.DataFrame(columns=["title", "authors", "venue", "pub_year", "url", "doi", "abstract", "keywords", "source"])  # empty

    all_df = pd.concat(frames, ignore_index=True)

    # íƒ€ì… ë³´ì •
    all_df["pub_year"] = all_df["pub_year"].astype(str).str.extract(r"(\d{4})")[0]

    # ì¤‘ë³µ ì œê±° (ìš°ì„  DOI, ê·¸ë‹¤ìŒ ì œëª© ì •ê·œí™”)
    before = len(all_df)
    if "doi" in all_df.columns:
        all_df = all_df.drop_duplicates(subset=["doi"]).reset_index(drop=True)
    # DOIê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ì œëª© í‚¤ ìƒì„±
    all_df["_title_key"] = all_df["title"].apply(normalize_title)
    all_df = all_df.drop_duplicates(subset=["_title_key"]).drop(columns=["_title_key"]).reset_index(drop=True)
    after = len(all_df)

    st.caption(f"ğŸ§¹ ì¤‘ë³µ ì œê±°: {before - after}ê±´ ì œê±°, ìµœì¢… {after}ê±´")
    return all_df


# --- ì‚¬ì´ë“œë°” ---
st.sidebar.title("âš™ï¸ ì˜µì…˜")
use_ai = st.sidebar.checkbox("AIë¡œ ê²€ìƒ‰ì–´ í™•ì¥ ì‚¬ìš©", value=bool(OPENAI_API_KEY))
source_kci = st.sidebar.checkbox("KCI ì‚¬ìš©", value=True)
source_riss = st.sidebar.checkbox("RISS ì‚¬ìš©", value=True)

col1, col2 = st.sidebar.columns(2)
with col1:
    start_year = st.number_input("ì‹œì‘ì—°ë„", min_value=1900, max_value=2100, value=2018)
with col2:
    end_year = st.number_input("ì¢…ë£Œì—°ë„", min_value=1900, max_value=2100, value=2025)

kci_fetch_details = st.sidebar.checkbox("KCI ìƒì„¸ì¡°íšŒ(ì´ˆë¡/í‚¤ì›Œë“œ ë³´ê°•)", value=True)
riss_doc_type = st.sidebar.selectbox("RISS ìë£Œìœ í˜•", options=["T(í•™ìœ„)", "A(êµ­ë‚´í•™ìˆ )", "F(í•´ì™¸í•™ìˆ )", "í˜¼í•©"], index=0)
max_riss = st.sidebar.slider("RISS ìµœëŒ€ ê²°ê³¼ ìˆ˜", min_value=50, max_value=500, value=200, step=50)

# --- ë©”ì¸ ì˜ì—­ ---
st.title("ğŸ” ì„ í–‰ì—°êµ¬ ìë™ë¶„ë¥˜ê¸° (MVP)")
st.write("KCI/RISSë¥¼ í†µí•©í•´ ìì—°ì–´ë¡œ ì„ í–‰ì—°êµ¬ë¥¼ ë¹ ë¥´ê²Œ ìˆ˜ì§‘í•˜ê³  ì •ì œí•©ë‹ˆë‹¤.")

query = st.text_input("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ) ì²­ë…„ì—ê²Œ ì‹ ìš© ì œì•½ì´ ë¯¸ì¹˜ëŠ” ì˜í–¥")
run = st.button("ê²€ìƒ‰ ì‹¤í–‰", type="primary")

if run:
    if not query.strip():
        st.warning("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        st.stop()

    # --- ê²€ìƒ‰ì–´ ì •ê·œí™” ---
    with st.spinner("ê²€ìƒ‰ì–´ ì •ê·œí™” ì¤‘â€¦"):
        try:
            normalizer = QueryNormalizer(
                csv_path="keyword_mapping.csv",
                use_ai=use_ai and bool(OPENAI_API_KEY),
                model="gpt-3.5-turbo",
            )
            # query_normalizer.py ë²„ì „ í˜¸í™˜ (êµ¬ë²„ì „: normalize(query), ì‹ ë²„ì „: normalize(query, max_total=..., include_original=...))
            try:
                terms = normalizer.normalize(query, max_total=10, include_original=True)
            except TypeError:
                # êµ¬ë²„ì „ ì‹œê·¸ë‹ˆì²˜ í˜¸í™˜
                terms = normalizer.normalize(query)
        except Exception as e:
            st.error(f"ê²€ìƒ‰ì–´ ì •ê·œí™” ì‹¤íŒ¨: {e}")
            terms = [query]

    st.success("ì •ê·œí™”ëœ í‚¤ì›Œë“œ")
    st.write(terms)

    primary_term = terms[0] if terms else query

    # --- ë°ì´í„° ìˆ˜ì§‘ ---
    df_kci = pd.DataFrame()
    df_riss = pd.DataFrame()

    # KCI
    if source_kci:
        if not KCI_API_KEY:
            st.warning("KCI_API_KEYê°€ .envì— ì—†ìŠµë‹ˆë‹¤. KCI ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        else:
            with st.spinner("KCI ê²€ìƒ‰ ì¤‘â€¦"):
                try:
                    kci = OptimizedKCIAnalyzer(api_key=KCI_API_KEY)
                    kci_result = kci.search_articles(
                        title=primary_term,
                        date_from=f"{int(start_year)}01",
                        date_to=f"{int(end_year)}12",
                        page=1,
                        display_count=50,
                    )
                    df_kci = kci.extract_article_info_optimized(kci_result, fetch_details=kci_fetch_details)
                    st.success(f"KCI ê²°ê³¼: {len(df_kci)}ê±´")
                except Exception as e:
                    st.error(f"KCI ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

    # RISS
    if source_riss:
        if not RISS_API_KEY:
            st.warning("RISS_API_KEYê°€ .envì— ì—†ìŠµë‹ˆë‹¤. RISS ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        else:
            with st.spinner("RISS ê²€ìƒ‰ ì¤‘â€¦"):
                try:
                    riss = ImprovedRISSAnalyzer(api_key=RISS_API_KEY)
                    dtype_map = {
                        "T(í•™ìœ„)": "T",
                        "A(êµ­ë‚´í•™ìˆ )": "A",
                        "F(í•´ì™¸í•™ìˆ )": "F",
                        "í˜¼í•©": None,
                    }
                    dtype = dtype_map.get(riss_doc_type)

                    results = []
                    if dtype is None:
                        for t in ["T", "A", "F"]:
                            results.extend(
                                riss.search_with_multiple_strategies(
                                    primary_term, doc_type=t, start_year=int(start_year), end_year=int(end_year), max_results=max_riss
                                )
                            )
                            time.sleep(0.2)
                    else:
                        results = riss.search_with_multiple_strategies(
                            primary_term, doc_type=dtype, start_year=int(start_year), end_year=int(end_year), max_results=max_riss
                        )
                    df_riss = riss.create_dataframe(results)
                    st.success(f"RISS ê²°ê³¼: {len(df_riss)}ê±´")
                except Exception as e:
                    st.error(f"RISS ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

    if (df_kci is None or df_kci.empty) and (df_riss is None or df_riss.empty):
        st.info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. í‚¤ì›Œë“œë¥¼ ë” ì¼ë°˜ì ìœ¼ë¡œ ë³€ê²½í•˜ê±°ë‚˜ ì—°ë„ ë²”ìœ„ë¥¼ ë„“í˜€ë³´ì„¸ìš”.")
        st.stop()

    # --- ë³‘í•©/ì¤‘ë³µ ì œê±° ---
    with st.spinner("ê²°ê³¼ ë³‘í•© ë° ì¤‘ë³µ ì œê±°â€¦"):
        merged = merge_frames(df_kci, df_riss)

    # --- í•„í„° UI ---
    st.subheader("ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°")
    with st.expander("í•„í„°", expanded=True):
        colf1, colf2, colf3 = st.columns(3)
        with colf1:
            src_filter = st.multiselect("ì†ŒìŠ¤", options=sorted(merged["source"].dropna().unique().tolist()), default=sorted(merged["source"].dropna().unique().tolist()))
        with colf2:
            years = sorted([y for y in merged["pub_year"].dropna().unique().tolist() if isinstance(y, str)])
            year_filter = st.multiselect("ì—°ë„", options=years, default=years)
        with colf3:
            text_filter = st.text_input("ì œëª©/ì €ì í¬í•¨ í‚¤ì›Œë“œ")

    view = merged.copy()
    if src_filter:
        view = view[view["source"].isin(src_filter)]
    if year_filter:
        view = view[view["pub_year"].astype(str).isin([str(y) for y in year_filter])]
    if text_filter.strip():
        ql = text_filter.lower().strip()
        view = view[
            view["title"].fillna("").str.lower().str.contains(ql)
            | view["authors"].fillna("").str.lower().str.contains(ql)
        ]

    st.dataframe(view.reset_index(drop=True), use_container_width=True, height=420)

    # --- ë‹¤ìš´ë¡œë“œ ---
    c1, c2, c3 = st.columns(3)
    with c1:
        st.download_button(
            "ğŸ“¥ CSV ë‹¤ìš´ë¡œë“œ",
            data=view.to_csv(index=False).encode("utf-8-sig"),
            file_name=f"results_{normalize_title(primary_term)}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv",
        )
    with c2:
        xls = to_excel_bytes(view)
        st.download_button(
            "ğŸ“Š Excel ë‹¤ìš´ë¡œë“œ",
            data=xls,
            file_name=f"results_{normalize_title(primary_term)}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        )
    with c3:
        st.write("")

    # --- ë¶„ì„ íƒ­ ---
    tab1, tab2, tab3 = st.tabs(["ì—°ë„ë³„ ë¶„í¬", "ì†ŒìŠ¤ë³„ ë¶„í¬", "í•™ìˆ ì§€/ê¸°ê´€ Top 15"])

    with tab1:
        st.subheader("ì—°ë„ë³„ ë…¼ë¬¸ ìˆ˜")
        ycnt = (
            view.assign(pub_year=view["pub_year"].astype(str).str.extract(r"(\d{4})")[0])
            .dropna(subset=["pub_year"]) 
            .groupby("pub_year").size()
        )
        if not ycnt.empty:
            fig = plt.figure()
            ycnt.sort_index().plot(kind="bar")
            plt.xlabel("ì—°ë„")
            plt.ylabel("ë…¼ë¬¸ ìˆ˜")
            plt.tight_layout()
            st.pyplot(fig)
        else:
            st.info("ì—°ë„ ì •ë³´ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    with tab2:
        st.subheader("ì†ŒìŠ¤ë³„ ë…¼ë¬¸ ìˆ˜")
        scnt = view.groupby("source").size()
        if not scnt.empty:
            fig = plt.figure()
            scnt.plot(kind="bar")
            plt.xlabel("ì†ŒìŠ¤")
            plt.ylabel("ë…¼ë¬¸ ìˆ˜")
            plt.tight_layout()
            st.pyplot(fig)
        else:
            st.info("ì†ŒìŠ¤ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")

    with tab3:
        st.subheader("í•™ìˆ ì§€/ê¸°ê´€ ìƒìœ„ 15")
        if "venue" in view.columns and not view["venue"].dropna().empty:
            vcnt = (
                view["venue"].fillna("").replace("", np.nan).dropna().value_counts().head(15)
            )
            if not vcnt.empty:
                fig = plt.figure()
                vcnt.sort_values(ascending=True).plot(kind="barh")
                plt.xlabel("ë…¼ë¬¸ ìˆ˜")
                plt.ylabel("í•™ìˆ ì§€/ê¸°ê´€")
                plt.tight_layout()
                st.pyplot(fig)
            else:
                st.info("í‘œì‹œí•  í•™ìˆ ì§€/ê¸°ê´€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("í•™ìˆ ì§€/ê¸°ê´€ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

else:
    st.info("ì¢Œì¸¡ ì˜µì…˜ì„ í™•ì¸í•˜ê³ , ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•œ ë’¤ 'ê²€ìƒ‰ ì‹¤í–‰'ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
