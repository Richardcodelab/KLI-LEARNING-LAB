import requests
import xml.etree.ElementTree as ET
import pandas as pd
import json
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import re
from urllib.parse import quote
import platform
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def setup_korean_font():
    """í•œê¸€ í°íŠ¸ ì„¤ì •"""
    import matplotlib.font_manager as fm
    
    system = platform.system()
    
    if system == 'Windows':
        font_names = ['Malgun Gothic', 'Microsoft YaHei', 'NanumGothic', 'SimHei', 'Arial Unicode MS']
    elif system == 'Darwin':  # macOS
        font_names = ['AppleGothic', 'Apple SD Gothic Neo', 'Helvetica', 'Arial Unicode MS']
    else:  # Linux
        font_names = ['NanumGothic', 'DejaVu Sans', 'Liberation Sans']
    
    available_fonts = [f.name for f in fm.fontManager.ttflist]
    
    selected_font = None
    for font_name in font_names:
        if font_name in available_fonts:
            selected_font = font_name
            plt.rcParams['font.family'] = font_name
            break
    
    if selected_font is None:
        plt.rcParams['font.family'] = 'DejaVu Sans'
        selected_font = 'DejaVu Sans'
        logger.warning("âš ï¸ í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    plt.rcParams['axes.unicode_minus'] = False
    
    try:
        fm._rebuild()
    except:
        pass
    
    logger.info(f"âœ… ì„¤ì •ëœ í°íŠ¸: {selected_font}")
    return selected_font


setup_korean_font()


class OptimizedKCIAnalyzer:
    def __init__(self, api_key):
        self.api_key = api_key
        self.base_url = "https://open.kci.go.kr/po/openapi/openApiSearch.kci"
        self.headers = {
            'Content-Type': 'application/json',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
        # ìºì‹œ ì €ì¥ì†Œ
        self.detail_cache = {}
        
    def search_articles(self, title=None, author=None, journal=None, 
                       date_from=None, date_to=None, page=1, display_count=10):
        """ë…¼ë¬¸ ê¸°ë³¸ ì •ë³´ ê²€ìƒ‰"""
        params = {
            'apiCode': 'articleSearch',
            'key': self.api_key,
            'page': page,
            'displayCount': min(display_count, 100)
        }
        
        if title:
            params['title'] = title
        if author:
            params['author'] = author
        if journal:
            params['journal'] = journal
        if date_from:
            params['dateFrom'] = date_from
        if date_to:
            params['dateTo'] = date_to
            
        logger.info(f"ğŸ” KCI API í˜¸ì¶œ (í˜ì´ì§€ {page}, {display_count}ê±´)...")
        
        try:
            response = self.session.get(self.base_url, params=params, timeout=15)
            
            if response.status_code == 200:
                root = ET.fromstring(response.text)
                result = self._xml_to_dict(root)
                logger.info(f"âœ… API í˜¸ì¶œ ì„±ê³µ")
                return result
            else:
                logger.error(f"âŒ HTTP ì˜¤ë¥˜: {response.status_code}")
                return None
            
        except requests.Timeout:
            logger.error("âŒ API ìš”ì²­ íƒ€ì„ì•„ì›ƒ")
            return None
        except Exception as e:
            logger.error(f"âŒ API ìš”ì²­ ì˜¤ë¥˜: {e}")
            return None
    
    def get_article_detail_batch(self, article_ids, max_workers=5):
        """ë°°ì¹˜ë¡œ ìƒì„¸ ì •ë³´ ì¡°íšŒ - ë³‘ë ¬ ì²˜ë¦¬"""
        details = {}
        
        def fetch_detail(article_id):
            if article_id in self.detail_cache:
                return article_id, self.detail_cache[article_id]
            
            params = {
                'apiCode': 'articleDetail',
                'key': self.api_key,
                'id': article_id
            }
            
            try:
                response = self.session.get(self.base_url, params=params, timeout=10)
                if response.status_code == 200:
                    root = ET.fromstring(response.text)
                    result = self._xml_to_dict(root)
                    self.detail_cache[article_id] = result
                    return article_id, result
                else:
                    return article_id, None
            except Exception as e:
                logger.warning(f"âš ï¸ ìƒì„¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨ ({article_id}): {e}")
                return article_id, None
        
        logger.info(f"ğŸ” {len(article_ids)}ê°œ ë…¼ë¬¸ ìƒì„¸ ì •ë³´ ë³‘ë ¬ ì¡°íšŒ...")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_id = {executor.submit(fetch_detail, aid): aid for aid in article_ids}
            
            for future in as_completed(future_to_id):
                article_id, result = future.result()
                if result:
                    details[article_id] = result
                time.sleep(0.1)  # API í˜¸ì¶œ ê°„ê²© ì¡°ì ˆ
        
        logger.info(f"âœ… {len(details)}ê°œ ìƒì„¸ ì •ë³´ ì¡°íšŒ ì™„ë£Œ")
        return details
    
    def _xml_to_dict(self, element):
        """ìµœì í™”ëœ XML to Dict ë³€í™˜"""
        result = {}
        
        if element.text and element.text.strip():
            result['text'] = element.text.strip()
        
        if element.attrib:
            result.update(element.attrib)
        
        for child in element:
            child_data = self._xml_to_dict(child)
            if child.tag in result:
                if not isinstance(result[child.tag], list):
                    result[child.tag] = [result[child.tag]]
                result[child.tag].append(child_data)
            else:
                result[child.tag] = child_data
        
        return result
    
    def _extract_text_fast(self, data):
        """ë¹ ë¥¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        if data is None:
            return ''
        
        if isinstance(data, str):
            return data.strip()
        
        if isinstance(data, dict):
            if 'text' in data:
                return str(data['text']).strip()
            
            for key in ['#text', 'content', 'value']:
                if key in data:
                    return str(data[key]).strip()
            
            if len(data) == 1:
                key, value = next(iter(data.items()))
                if key not in ['lang', 'type', 'id']:
                    return self._extract_text_fast(value)
            
            return ''
        
        if isinstance(data, list) and data:
            return self._extract_text_fast(data[0])
        
        return str(data).strip() if data else ''
    
    def _extract_keywords_optimized(self, article_info, detail_info=None):
        """ìµœì í™”ëœ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keyword_sources = [
            (article_info, ['keyword-group', 'keyword']),
            (article_info, ['kwd-group', 'kwd']),
            (article_info, ['keywords']),
            (article_info, ['keyword']),
        ]
        
        # ìƒì„¸ ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if detail_info:
            detail_article_info = detail_info.get('outputData', {}).get('record', {}).get('articleInfo', {})
            keyword_sources.extend([
                (detail_article_info, ['keyword-group', 'keyword']),
                (detail_article_info, ['kwd-group', 'kwd']),
            ])
        
        for source_data, path in keyword_sources:
            keywords = self._extract_from_path(source_data, path)
            if keywords:
                return '; '.join(keywords)
        
        return ''
    
    def _extract_from_path(self, data, path):
        """ê²½ë¡œë¥¼ ë”°ë¼ ë°ì´í„° ì¶”ì¶œ"""
        current = data
        
        for key in path[:-1]:
            if isinstance(current, dict) and key in current:
                current = current[key]
            else:
                return []
        
        if not isinstance(current, dict) or path[-1] not in current:
            return []
        
        keyword_data = current[path[-1]]
        keywords = []
        
        if isinstance(keyword_data, list):
            for item in keyword_data:
                text = self._extract_text_fast(item)
                if text:
                    keywords.append(text)
        else:
            text = self._extract_text_fast(keyword_data)
            if text:
                keywords.append(text)
        
        return keywords

    def _extract_abstract_optimized(self, article_info):
        """ìƒì„¸ ì‘ë‹µ(articleInfo)ì—ì„œ ì´ˆë¡ í…ìŠ¤íŠ¸ë¥¼ ìµœëŒ€í•œ íƒ„ë ¥ì ìœ¼ë¡œ ì¶”ì¶œ"""
        if not isinstance(article_info, dict):
            return ''

        candidates = [
            ['abstract'],
            ['abstract-group', 'abstract'],
            ['abstract', 'p'],
            ['abstract', 'text'],
            ['abstract-group', 'abstract', 'p'],
            ['abstract-group', 'abstract', 'text'],
        ]

        for path in candidates:
            cur = article_info
            ok = True
            for key in path:
                if isinstance(cur, dict) and key in cur:
                    cur = cur[key]
                else:
                    ok = False
                    break
            if ok:
                txt = self._extract_text_fast(cur)
                if txt:
                    return txt
        return ''
    
    def extract_article_info_optimized(self, api_response, fetch_details=True, detail_batch_size=10):
        """ìµœì í™”ëœ ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ"""
        if not api_response:
            return pd.DataFrame()
        
        start_time = time.time()
        articles = []
        
        try:
            output_data = api_response.get('outputData', {})
            records = output_data.get('record', [])
            
            if isinstance(records, dict):
                records = [records]
            
            logger.info(f"ğŸ“Š {len(records)}ê°œ ë…¼ë¬¸ ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
            
            # 1ë‹¨ê³„: ê¸°ë³¸ ì •ë³´ ë¹ ë¥´ê²Œ ì¶”ì¶œ
            article_ids_need_detail = []
            
            for i, record in enumerate(records):
                article = self._extract_basic_info(record)
                articles.append(article)
                
                # í‚¤ì›Œë“œë‚˜ ì´ˆë¡ì´ ì—†ìœ¼ë©´ ìƒì„¸ ì¡°íšŒ
                needs_detail = (
                    (not article.get('keywords')) or
                    (not article.get('abstract'))
                )
                if needs_detail and article.get('article_id') and fetch_details:
                    article_ids_need_detail.append((i, article['article_id']))
            
            logger.info(f"âš¡ 1ë‹¨ê³„ ì™„ë£Œ: {len(articles)}ê°œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ ({time.time() - start_time:.1f}ì´ˆ)")
            
            # 2ë‹¨ê³„: ìƒì„¸ ì¡°íšŒë¡œ í‚¤ì›Œë“œ/ì´ˆë¡ ë³´ê°•
            if article_ids_need_detail:
                batch_start = time.time()
                ids_only = [aid for _, aid in article_ids_need_detail]
                details = self.get_article_detail_batch(ids_only, max_workers=5)
                
                for i, article_id in article_ids_need_detail:
                    if article_id in details:
                        detail_info = details[article_id]
                        # í‚¤ì›Œë“œ ë³´ê°•
                        enhanced_keywords = self._extract_keywords_optimized(
                            records[i].get('articleInfo', {}), detail_info
                        )
                        if enhanced_keywords and not articles[i].get('keywords'):
                            articles[i]['keywords'] = enhanced_keywords

                        # ì´ˆë¡ ë³´ê°•
                        if not articles[i].get('abstract'):
                            detail_article_info = detail_info.get('outputData', {}).get('record', {}).get('articleInfo', {})
                            enhanced_abs = self._extract_abstract_optimized(detail_article_info)
                            if enhanced_abs:
                                articles[i]['abstract'] = enhanced_abs
                
                logger.info(f"âš¡ 2ë‹¨ê³„ ì™„ë£Œ: {len(details)}ê°œ ìƒì„¸ ì •ë³´ ë³´ê°• ({time.time() - batch_start:.1f}ì´ˆ)")
            
            logger.info(f"ğŸš€ ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ: {time.time() - start_time:.1f}ì´ˆ")
            
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
        
        return pd.DataFrame(articles)
    
    def _extract_basic_info(self, record):
        """ê¸°ë³¸ ì •ë³´ ë¹ ë¥¸ ì¶”ì¶œ"""
        article = {}
        
        # ì €ë„ ì •ë³´
        journal_info = record.get('journalInfo', {})
        article['journal_name'] = self._extract_text_fast(journal_info.get('journal-name', {}))
        article['publisher_name'] = self._extract_text_fast(journal_info.get('publisher-name', {}))
        article['pub_year'] = self._extract_text_fast(journal_info.get('pub-year', {}))
        article['pub_mon'] = self._extract_text_fast(journal_info.get('pub-mon', {}))
        article['volume'] = self._extract_text_fast(journal_info.get('volume', {}))
        article['issue'] = self._extract_text_fast(journal_info.get('issue', {}))
        
        # ë…¼ë¬¸ ì •ë³´
        article_info = record.get('articleInfo', {})
        article['article_id'] = article_info.get('article-id', '')
        article['categories'] = self._extract_text_fast(article_info.get('article-categories', {}))
        article['regularity'] = self._extract_text_fast(article_info.get('article-regularity', {}))
        
        # ì œëª©
        title_group = article_info.get('title-group', {})
        article_title = title_group.get('article-title', {})
        
        if isinstance(article_title, list):
            title_text = self._extract_text_fast(article_title[0])
        else:
            title_text = self._extract_text_fast(article_title)
        
        article['title'] = title_text or 'ì œëª© ì—†ìŒ'
        
        # ì €ì
        author_group = article_info.get('author-group', {})
        authors = author_group.get('author', [])
        if isinstance(authors, dict):
            authors = [authors]
        
        author_names = []
        for author in authors[:5]:  # ìµœëŒ€ 5ëª…ë§Œ ì²˜ë¦¬
            author_text = self._extract_text_fast(author)
            if author_text:
                author_names.append(author_text)
        
        article['authors'] = '; '.join(author_names)
        
        # ì´ˆë¡
        abstract_text = self._extract_text_fast(article_info.get('abstract', {}))
        if not abstract_text:
            abstract_group = article_info.get('abstract-group', {})
            if abstract_group:
                abstract = abstract_group.get('abstract', {})
                abstract_text = self._extract_text_fast(abstract)
        
        article['abstract'] = abstract_text
        
        # í‚¤ì›Œë“œ
        article['keywords'] = self._extract_keywords_optimized(article_info)
        
        # ê¸°íƒ€ ì •ë³´
        article['fpage'] = self._extract_text_fast(article_info.get('fpage', {}))
        article['lpage'] = self._extract_text_fast(article_info.get('lpage', {}))
        article['doi'] = self._extract_text_fast(article_info.get('doi', {}))
        article['uci'] = self._extract_text_fast(article_info.get('uci', {}))
        
        citation_count = article_info.get('citation-count', {})
        article['citation_count'] = self._extract_text_fast(citation_count) or '0'
        article['kci_citations'] = citation_count.get('kci', '0')
        article['wos_citations'] = citation_count.get('wos', '0')
        
        article['url'] = self._extract_text_fast(article_info.get('url', {}))
        article['verified'] = self._extract_text_fast(article_info.get('verified', {}))
        article['orte_open_yn'] = self._extract_text_fast(article_info.get('orte-open-yn', {}))
        
        return article
    
    def analyze_by_year(self, df):
        """ì—°ë„ë³„ ë…¼ë¬¸ ë°œí–‰ ë¶„ì„"""
        if df.empty or 'pub_year' not in df.columns:
            return pd.Series()
        
        df_copy = df.copy()
        df_copy['pub_year'] = pd.to_numeric(df_copy['pub_year'], errors='coerce')
        df_filtered = df_copy[df_copy['pub_year'].notna() & 
                             (df_copy['pub_year'] >= 1900) & 
                             (df_copy['pub_year'] <= 2025)]
        
        return df_filtered['pub_year'].value_counts().sort_index()
    
    def analyze_journals(self, df, top_n=10):
        """í•™ìˆ ì§€ë³„ ë…¼ë¬¸ ìˆ˜ ë¶„ì„"""
        if df.empty or 'journal_name' not in df.columns:
            return pd.Series()
        
        return df[df['journal_name'] != '']['journal_name'].value_counts().head(top_n)
    
    def analyze_categories(self, df, top_n=10):
        """ì—°êµ¬ë¶„ì•¼ë³„ ë…¼ë¬¸ ìˆ˜ ë¶„ì„"""
        if df.empty or 'categories' not in df.columns:
            return pd.Series()
        
        return df[df['categories'] != '']['categories'].value_counts().head(top_n)
    
    def analyze_keywords(self, df, top_n=15):
        """í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„"""
        if df.empty or 'keywords' not in df.columns:
            return pd.Series()
        
        all_keywords = []
        for keywords_str in df['keywords'].dropna():
            if keywords_str and str(keywords_str).strip():
                keywords = re.split('[;,/]', str(keywords_str))
                for kw in keywords:
                    kw = kw.strip()
                    if kw and len(kw) > 1:
                        all_keywords.append(kw)
        
        if all_keywords:
            keyword_counts = pd.Series(all_keywords).value_counts().head(top_n)
            return keyword_counts
        else:
            return pd.Series()
    
    def export_to_excel(self, df, filename, search_term):
        """Excel íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        try:
            with pd.ExcelWriter(filename, engine='openpyxl') as writer:
                # ë…¼ë¬¸ ë°ì´í„°
                df.to_excel(writer, sheet_name='ë…¼ë¬¸_ë°ì´í„°', index=False)
                
                # ì—°ë„ë³„ ë¶„ì„
                year_analysis = self.analyze_by_year(df)
                if not year_analysis.empty:
                    year_df = pd.DataFrame({'ì—°ë„': year_analysis.index, 'ë…¼ë¬¸ìˆ˜': year_analysis.values})
                    year_df.to_excel(writer, sheet_name='ì—°ë„ë³„_ë¶„ì„', index=False)
                
                # í•™ìˆ ì§€ë³„ ë¶„ì„
                journal_analysis = self.analyze_journals(df)
                if not journal_analysis.empty:
                    journal_df = pd.DataFrame({'í•™ìˆ ì§€': journal_analysis.index, 'ë…¼ë¬¸ìˆ˜': journal_analysis.values})
                    journal_df.to_excel(writer, sheet_name='í•™ìˆ ì§€ë³„_ë¶„ì„', index=False)
                
                # í‚¤ì›Œë“œ ë¶„ì„
                keyword_analysis = self.analyze_keywords(df)
                if not keyword_analysis.empty:
                    keyword_df = pd.DataFrame({'í‚¤ì›Œë“œ': keyword_analysis.index, 'ë¹ˆë„': keyword_analysis.values})
                    keyword_df.to_excel(writer, sheet_name='í‚¤ì›Œë“œ_ë¶„ì„', index=False)
            
            logger.info(f"âœ… ë°ì´í„°ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    api_key = "30354185"
    analyzer = OptimizedKCIAnalyzer(api_key)
    
    result = analyzer.search_articles(
        title="ê³ ìš©",
        date_from="201501",
        date_to="202512",
        page=1,
        display_count=50
    )
    
    if result:
        df = analyzer.extract_article_info_optimized(result, fetch_details=True)
        print(f"\nâœ… ì¶”ì¶œëœ ë…¼ë¬¸ ìˆ˜: {len(df)}ê±´")
        print(df.head())