import requests
import xml.etree.ElementTree as ET
import pandas as pd
import json
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import re
from urllib.parse import quote, urlencode
import platform
import time
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def setup_korean_font():
    """í•œê¸€ í°íŠ¸ ì„¤ì •"""
    import matplotlib.font_manager as fm
    
    system = platform.system()
    
    if system == 'Windows':
        font_names = ['Malgun Gothic', 'Microsoft YaHei', 'NanumGothic', 'SimHei', 'Arial Unicode MS']
    elif system == 'Darwin':  # macOS
        font_names = ['AppleGothic', 'Apple SD Gothic Neo', 'Helvetica', 'Arial Unicode MS']
    else:  # Linux
        font_names = ['NanumGothic', 'DejaVu Sans', 'Liberation Sans']
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ í°íŠ¸ ì°¾ê¸°
    available_fonts = [f.name for f in fm.fontManager.ttflist]
    
    selected_font = None
    for font_name in font_names:
        if font_name in available_fonts:
            selected_font = font_name
            plt.rcParams['font.family'] = font_name
            break
    
    if selected_font is None:
        # ê¸°ë³¸ í°íŠ¸ë¡œ ì„¤ì •
        plt.rcParams['font.family'] = 'DejaVu Sans'
        selected_font = 'DejaVu Sans'
        logger.warning("âš ï¸ í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    plt.rcParams['axes.unicode_minus'] = False
    
    # í°íŠ¸ ìºì‹œ ìƒˆë¡œê³ ì¹¨
    try:
        fm._rebuild()
    except:
        pass
    
    logger.info(f"âœ… ì„¤ì •ëœ í°íŠ¸: {selected_font}")
    return selected_font


setup_korean_font()


class ImprovedRISSAnalyzer:
    def __init__(self, api_key=None):
        """ê°œì„ ëœ RISS API ë¶„ì„ í´ë˜ìŠ¤"""
        self.api_key = api_key or '70aaa00wqm60acd00aaa00ab01za061a'
        self.base_url = "http://www.riss.kr/openApi"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/xml,text/xml,*/*',
            'Accept-Language': 'ko-KR,ko;q=0.8',
            'Connection': 'keep-alive'
        }
        self.all_results = []  # ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ ì €ì¥
        self.session = requests.Session()
        self.session.headers.update(self.headers)
    
    def _extract_text(self, data):
        """ë°ì´í„°ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            if isinstance(data, dict):
                if 'text' in data:
                    return str(data['text']).strip()
                elif '#text' in data:
                    return str(data['#text']).strip()
                elif data:
                    return str(list(data.values())[0]).strip()
                else:
                    return ''
            elif isinstance(data, str):
                return data.strip()
            elif isinstance(data, list) and data:
                return self._extract_text(data[0])
            elif data is None:
                return ''
            else:
                return str(data).strip()
        except Exception as e:
            logger.warning(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return ''
    
    def search_with_multiple_strategies(self, search_term, doc_type='T', 
                                       start_year=2015, end_year=2025, max_results=200):
        """
        ë‹¤ì¤‘ ê²€ìƒ‰ ì „ëµì„ ì‚¬ìš©í•œ ë…¼ë¬¸ ê²€ìƒ‰
        1. ì œëª© ì „ìš© ê²€ìƒ‰ (ì •í™•ë„ ë†’ìŒ)
        2. í‚¤ì›Œë“œ ì „ì²´ ê²€ìƒ‰ (ë²”ìœ„ ë„“ìŒ)  
        3. ì—°ë„ë³„ ë¶„í•  ê²€ìƒ‰ (ê°œìˆ˜ ì œí•œ ìš°íšŒ)
        """
        logger.info(f"=== ê°œì„ ëœ RISS ê²€ìƒ‰ ì‹œì‘ ===")
        logger.info(f"ê²€ìƒ‰ì–´: {search_term}")
        logger.info(f"ê¸°ê°„: {start_year}-{end_year}")
        logger.info(f"ëª©í‘œ ê°œìˆ˜: {max_results}")
        
        all_results = []
        search_stats = {
            'title_search': 0,
            'keyword_search': 0,
            'yearly_search': 0,
            'errors': []
        }
        
        # ì „ëµ 1: ì œëª© ì „ìš© ê²€ìƒ‰ (ë†’ì€ ì •í™•ë„)
        logger.info("\nğŸ¯ ì „ëµ 1: ì œëª© ì „ìš© ê²€ìƒ‰")
        title_results = self._search_by_title(search_term, doc_type, start_year, end_year, 100)
        if title_results:
            all_results.extend(title_results)
            search_stats['title_search'] = len(title_results)
            logger.info(f"ì œëª© ê²€ìƒ‰ ê²°ê³¼: {len(title_results)}ê±´")
        
        # ì „ëµ 2: í‚¤ì›Œë“œ ì „ì²´ ê²€ìƒ‰ (ë„“ì€ ë²”ìœ„)
        if len(all_results) < max_results:
            logger.info("\nğŸ” ì „ëµ 2: í‚¤ì›Œë“œ ì „ì²´ ê²€ìƒ‰")
            keyword_results = self._search_by_keyword(
                search_term, doc_type, start_year, end_year, 
                max_results - len(all_results)
            )
            if keyword_results:
                # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
                existing_urls = {r.get('url', '') for r in all_results}
                new_results = [r for r in keyword_results if r.get('url', '') not in existing_urls]
                all_results.extend(new_results)
                search_stats['keyword_search'] = len(new_results)
                logger.info(f"í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼: {len(keyword_results)}ê±´ (ì¤‘ë³µ ì œê±° í›„: {len(new_results)}ê±´)")
        
        # ì „ëµ 3: ì—°ë„ë³„ ë¶„í•  ê²€ìƒ‰ (API ì œí•œ ìš°íšŒ)
        if len(all_results) < max_results:
            logger.info("\nğŸ“… ì „ëµ 3: ì—°ë„ë³„ ë¶„í•  ê²€ìƒ‰")
            yearly_results = self._search_by_years(
                search_term, doc_type, start_year, end_year, 
                max_results - len(all_results)
            )
            if yearly_results:
                existing_urls = {r.get('url', '') for r in all_results}
                new_results = [r for r in yearly_results if r.get('url', '') not in existing_urls]
                all_results.extend(new_results)
                search_stats['yearly_search'] = len(new_results)
                logger.info(f"ì—°ë„ë³„ ê²€ìƒ‰ ê²°ê³¼: {len(yearly_results)}ê±´ (ì¤‘ë³µ ì œê±° í›„: {len(new_results)}ê±´)")
        
        logger.info(f"\nâœ… ì´ ê²€ìƒ‰ ê²°ê³¼: {len(all_results)}ê±´")
        logger.info(f"   - ì œëª© ê²€ìƒ‰: {search_stats['title_search']}ê±´")
        logger.info(f"   - í‚¤ì›Œë“œ ê²€ìƒ‰: {search_stats['keyword_search']}ê±´")
        logger.info(f"   - ì—°ë„ë³„ ê²€ìƒ‰: {search_stats['yearly_search']}ê±´")
        
        self.all_results = all_results
        return all_results
    
    def _search_by_title(self, search_term, doc_type, start_year, end_year, count):
        """ì œëª© ì „ìš© ê²€ìƒ‰"""
        params = {
            'key': self.api_key,
            'version': '1.0',
            'type': doc_type,
            'title': search_term,  # ì œëª©ì—ì„œë§Œ ê²€ìƒ‰
            'spubdate': start_year,
            'epubdate': end_year,
            'sort': 'Y',
            'asc': 'D',
            'rsnum': 1,
            'rowcount': min(count, 100)
        }
        
        return self._execute_search(params, "ì œëª© ê²€ìƒ‰")
    
    def _search_by_keyword(self, search_term, doc_type, start_year, end_year, count):
        """í‚¤ì›Œë“œ ì „ì²´ ê²€ìƒ‰"""
        params = {
            'key': self.api_key,
            'version': '1.0',
            'type': doc_type,
            'keyword': search_term,  # ì „ì²´ì—ì„œ ê²€ìƒ‰
            'spubdate': start_year,
            'epubdate': end_year,
            'sort': 'Y',
            'asc': 'D',
            'rsnum': 1,
            'rowcount': min(count, 100)
        }
        
        return self._execute_search(params, "í‚¤ì›Œë“œ ê²€ìƒ‰")
    
    def _search_by_years(self, search_term, doc_type, start_year, end_year, remaining_count):
        """ì—°ë„ë³„ ë¶„í•  ê²€ìƒ‰ (API ì œí•œ ìš°íšŒ)"""
        results = []
        per_year = max(10, remaining_count // (end_year - start_year + 1))
        
        for year in range(start_year, end_year + 1):
            if len(results) >= remaining_count:
                break
                
            params = {
                'key': self.api_key,
                'version': '1.0', 
                'type': doc_type,
                'keyword': search_term,
                'spubdate': year,
                'epubdate': year,
                'sort': 'Y',
                'asc': 'D',
                'rsnum': 1,
                'rowcount': min(per_year, 100)
            }
            
            year_results = self._execute_search(params, f"{year}ë…„ ê²€ìƒ‰", verbose=False)
            if year_results:
                results.extend(year_results)
                logger.info(f"  {year}ë…„: {len(year_results)}ê±´")
                time.sleep(0.5)  # API í˜¸ì¶œ ê°„ê²©
        
        return results
    
    def _execute_search(self, params, search_type, verbose=True):
        """ì‹¤ì œ API í˜¸ì¶œ ì‹¤í–‰"""
        try:
            if verbose:
                logger.info(f"  {search_type} ì‹¤í–‰ì¤‘...")
            
            response = self.session.get(
                self.base_url, 
                params=params, 
                timeout=15
            )
            
            if response.status_code == 200:
                # XML íŒŒì‹±
                root = ET.fromstring(response.text)
                result = self._xml_to_dict(root)
                
                # ê²°ê³¼ ì¶”ì¶œ
                articles = self._extract_articles_from_response(result)
                return articles
            else:
                logger.error(f"    âŒ HTTP ì˜¤ë¥˜: {response.status_code}")
                return []
                
        except requests.Timeout:
            logger.error(f"    âŒ {search_type} íƒ€ì„ì•„ì›ƒ")
            return []
        except ET.ParseError as e:
            logger.error(f"    âŒ XML íŒŒì‹± ì˜¤ë¥˜: {e}")
            return []
        except Exception as e:
            logger.error(f"    âŒ {search_type} ì˜¤ë¥˜: {e}")
            return []
    
    def _xml_to_dict(self, element):
        """XMLì„ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        result = {}
        
        if element.text and element.text.strip():
            result['text'] = element.text.strip()
        
        result.update(element.attrib)
        
        for child in element:
            child_data = self._xml_to_dict(child)
            if child.tag in result:
                if not isinstance(result[child.tag], list):
                    result[child.tag] = [result[child.tag]]
                result[child.tag].append(child_data)
            else:
                result[child.tag] = child_data
        
        return result
    
    def _extract_articles_from_response(self, api_response):
        """API ì‘ë‹µì—ì„œ ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ"""
        if not api_response:
            return []
        
        articles = []
        
        try:
            if 'head' in api_response and 'metadata' in api_response:
                head = api_response['head']
                total_count = self._extract_text(head.get('totalcount', ''))
                error = self._extract_text(head.get('Error', ''))
                
                if error != '0':
                    error_msg = self._extract_text(head.get('ErrorMessage', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'))
                    logger.warning(f"API ì˜¤ë¥˜: {error_msg}")
                    return []
                
                metadata = api_response['metadata']
                if isinstance(metadata, dict):
                    metadata = [metadata]
                
                for record in metadata:
                    try:
                        article = {
                            'title': self._extract_text(record.get('riss.title', '')),
                            'author': self._extract_text(record.get('riss.author', '')),
                            'publisher': self._extract_text(record.get('riss.publisher', '')),
                            'pub_year': self._extract_text(record.get('riss.pubdate', '')),
                            'doc_type': self._extract_text(record.get('riss.type', '')),
                            'material_type': self._extract_text(record.get('riss.mtype', '')),
                            'url': self._extract_text(record.get('url', '')),
                            'has_abstract': self._extract_text(record.get('riss.abstract', '')),
                            'has_toc': self._extract_text(record.get('riss.toc', '')),
                            'has_image': self._extract_text(record.get('riss.image', ''))
                        }
                        
                        # ìë£Œ ìœ í˜• ë³€í™˜
                        doc_type_map = {
                            'T': 'í•™ìœ„ë…¼ë¬¸',
                            'A': 'êµ­ë‚´í•™ìˆ ë…¼ë¬¸', 
                            'F': 'í•´ì™¸í•™ìˆ ë…¼ë¬¸'
                        }
                        article['doc_type_name'] = doc_type_map.get(
                            article['doc_type'], 
                            article['doc_type']
                        )
                        
                        # ë¹ˆ ê°’ ì²˜ë¦¬
                        for key in article:
                            if article[key] is None:
                                article[key] = ''
                        
                        # ì œëª©ì´ ì—†ëŠ” ë…¼ë¬¸ì€ ì œì™¸
                        if article['title']:
                            articles.append(article)
                        
                    except Exception as e:
                        logger.warning(f"ë…¼ë¬¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                        continue
                        
        except Exception as e:
            logger.error(f"ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            
        return articles
    
    def analyze_search_quality(self, search_term, results):
        """ê²€ìƒ‰ í’ˆì§ˆ ë¶„ì„"""
        if not results:
            return {}
        
        # ê²€ìƒ‰ì–´ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
        core_keywords = [search_term.lower()]
        
        # CSVì—ì„œ ë™ì˜ì–´ ê°€ì ¸ì˜¤ê¸° (ê°„ë‹¨ ë²„ì „)
        keyword_variants = {
            'ê³ ìš©': ['ê³ ìš©', 'ì·¨ì—…', 'ì¼ìë¦¬', 'ì±„ìš©', 'ì§ì—…', 'ê·¼ë¡œ', 'ê·¼ë¬´', 'ì§ì¥', 'ë…¸ë™', 
                   'employment', 'job', 'work', 'career', 'occupation', 'labor'],
            'êµìœ¡': ['êµìœ¡', 'í•™ìŠµ', 'êµìˆ˜', 'í•™êµ', 'ëŒ€í•™', 'ì—°ìˆ˜', 'í›ˆë ¨', 'ê°•ì˜', 'ìˆ˜ì—…',
                   'education', 'learning', 'teaching', 'school', 'university'],
            'ì˜ë£Œ': ['ì˜ë£Œ', 'ë³‘ì›', 'ì¹˜ë£Œ', 'ì§„ë£Œ', 'ê°„í˜¸', 'ì˜ì‚¬', 'í™˜ì', 'ê±´ê°•', 'ì§ˆë³‘',
                   'medical', 'hospital', 'treatment', 'healthcare', 'doctor'],
        }
        
        # ê²€ìƒ‰ì–´ì— í•´ë‹¹í•˜ëŠ” ë³€í˜•ì–´ ì°¾ê¸°
        related_keywords = []
        for key, variants in keyword_variants.items():
            if key in search_term.lower():
                related_keywords.extend([v.lower() for v in variants])
        
        if not related_keywords:
            related_keywords = core_keywords
        
        related_count = 0
        year_distribution = {}
        
        for paper in results:
            title_lower = str(paper.get('title', '')).lower()
            author_lower = str(paper.get('author', '')).lower()
            
            # ê´€ë ¨ì„± ì²´í¬
            if any(keyword in title_lower or keyword in author_lower 
                   for keyword in related_keywords):
                related_count += 1
            
            # ì—°ë„ ë¶„í¬
            year = paper.get('pub_year', '')
            if year:
                # ì—°ë„ ì¶”ì¶œ (YYYY í˜•ì‹)
                year_match = re.search(r'(\d{4})', str(year))
                if year_match:
                    year = year_match.group(1)
                    year_distribution[year] = year_distribution.get(year, 0) + 1
        
        analysis = {
            'total_papers': len(results),
            'related_papers': related_count,
            'relevance_rate': (related_count / len(results)) * 100 if results else 0,
            'year_distribution': year_distribution,
            'year_range': (
                f"{min(year_distribution.keys()) if year_distribution else 'N/A'} - "
                f"{max(year_distribution.keys()) if year_distribution else 'N/A'}"
            )
        }
        
        return analysis
    
    def create_dataframe(self, results):
        """ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜"""
        return pd.DataFrame(results)
    
    def analyze_by_year(self, df):
        """ì—°ë„ë³„ ë…¼ë¬¸ ë°œí–‰ ë¶„ì„"""
        if df.empty or 'pub_year' not in df.columns:
            return pd.Series()
        
        df_copy = df.copy()
        df_copy['pub_year'] = df_copy['pub_year'].astype(str)
        df_copy['pub_year'] = df_copy['pub_year'].str.extract(r'(\d{4})').iloc[:, 0]
        df_copy['pub_year'] = pd.to_numeric(df_copy['pub_year'], errors='coerce')
        
        df_filtered = df_copy[
            df_copy['pub_year'].notna() & 
            (df_copy['pub_year'] >= 1900) & 
            (df_copy['pub_year'] <= 2025)
        ]
        
        return df_filtered['pub_year'].value_counts().sort_index()
    
    def analyze_publishers(self, df, top_n=10):
        """ì¶œíŒì‚¬/ê¸°ê´€ë³„ ë…¼ë¬¸ ìˆ˜ ë¶„ì„"""
        if df.empty or 'publisher' not in df.columns:
            return pd.Series()
        
        return df[df['publisher'] != '']['publisher'].value_counts().head(top_n)
    
    def analyze_material_types(self, df, top_n=10):
        """ì„¸ë¶€ ìë£Œ ìœ í˜•ë³„ ë¶„ì„ (êµ­ë‚´ë°•ì‚¬, êµ­ë‚´ì„ì‚¬ ë“±)"""
        if df.empty or 'material_type' not in df.columns:
            return pd.Series()
        
        return df[df['material_type'] != '']['material_type'].value_counts().head(top_n)
    
    def analyze_doc_types(self, df, top_n=10):
        """ìë£Œ ìœ í˜•ë³„ ë¶„ì„"""
        if df.empty or 'doc_type_name' not in df.columns:
            return pd.Series()
        
        return df[df['doc_type_name'] != '']['doc_type_name'].value_counts().head(top_n)
    
    def export_results(self, results, filename, search_term):
        """ê²°ê³¼ë¥¼ Excelë¡œ ë‚´ë³´ë‚´ê¸°"""
        if not results:
            logger.warning("ë‚´ë³´ë‚¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        df = pd.DataFrame(results)
        analysis = self.analyze_search_quality(search_term, results)
        
        try:
            with pd.ExcelWriter(filename, engine='openpyxl') as writer:
                # ë…¼ë¬¸ ë°ì´í„°
                df.to_excel(writer, sheet_name='ë…¼ë¬¸_ë°ì´í„°', index=False)
                
                # ê²€ìƒ‰ í’ˆì§ˆ ë¶„ì„
                quality_df = pd.DataFrame([analysis])
                quality_df.to_excel(writer, sheet_name='ê²€ìƒ‰_í’ˆì§ˆ_ë¶„ì„', index=False)
                
                # ì—°ë„ë³„ ë¶„í¬
                if analysis['year_distribution']:
                    year_df = pd.DataFrame(
                        list(analysis['year_distribution'].items()), 
                        columns=['ì—°ë„', 'ë…¼ë¬¸ìˆ˜']
                    )
                    year_df = year_df.sort_values('ì—°ë„')
                    year_df.to_excel(writer, sheet_name='ì—°ë„ë³„_ë¶„í¬', index=False)
                
                # ì¶œíŒê¸°ê´€ë³„ ë¶„í¬
                publisher_analysis = self.analyze_publishers(df)
                if not publisher_analysis.empty:
                    pub_df = pd.DataFrame({
                        'ì¶œíŒê¸°ê´€': publisher_analysis.index, 
                        'ë…¼ë¬¸ìˆ˜': publisher_analysis.values
                    })
                    pub_df.to_excel(writer, sheet_name='ì¶œíŒê¸°ê´€ë³„_ë¶„ì„', index=False)
            
            logger.info(f"âœ… ë°ì´í„°ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")


def main():
    """ê°œì„ ëœ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ê²€ìƒ‰ ì„¤ì •
    search_term = "ê³ ìš©"
    doc_type = "T"  # T: í•™ìœ„ë…¼ë¬¸
    start_year = 2015
    end_year = 2025
    max_results = 200
    
    logger.info("=== ê°œì„ ëœ RISS ë…¼ë¬¸ ê²€ìƒ‰ ë„êµ¬ ===")
    logger.info(f"ê²€ìƒ‰ì–´: {search_term}")
    logger.info(f"ìë£Œìœ í˜•: {'í•™ìœ„ë…¼ë¬¸' if doc_type == 'T' else 'í•™ìˆ ë…¼ë¬¸'}")
    logger.info(f"ê²€ìƒ‰ê¸°ê°„: {start_year}-{end_year}")
    logger.info(f"ëª©í‘œê±´ìˆ˜: {max_results}\n")
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = ImprovedRISSAnalyzer()
    
    # ë‹¤ì¤‘ ì „ëµ ê²€ìƒ‰ ì‹¤í–‰
    start_time = time.time()
    results = analyzer.search_with_multiple_strategies(
        search_term, doc_type, start_year, end_year, max_results
    )
    search_time = time.time() - start_time
    
    if results:
        # ê²€ìƒ‰ í’ˆì§ˆ ë¶„ì„
        analysis = analyzer.analyze_search_quality(search_term, results)
        
        logger.info(f"\n=== ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„ ===")
        logger.info(f"ì´ ë…¼ë¬¸ ìˆ˜: {analysis['total_papers']}ê±´")
        logger.info(f"ê´€ë ¨ ë…¼ë¬¸ ìˆ˜: {analysis['related_papers']}ê±´")
        logger.info(f"ê²€ìƒ‰ ì •í™•ë„: {analysis['relevance_rate']:.1f}%")
        logger.info(f"ì—°ë„ ë²”ìœ„: {analysis['year_range']}")
        logger.info(f"ê²€ìƒ‰ ì†Œìš” ì‹œê°„: {search_time:.1f}ì´ˆ")
        
        # ìƒìœ„ ë…¼ë¬¸ ë¯¸ë¦¬ë³´ê¸°
        logger.info(f"\n=== ìƒìœ„ ë…¼ë¬¸ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 5ê±´) ===")
        for i, paper in enumerate(results[:5], 1):
            title = paper.get('title', 'N/A')
            # ê´€ë ¨ í‚¤ì›Œë“œ í•˜ì´ë¼ì´íŠ¸
            if any(kw in title.lower() for kw in ['ê³ ìš©', 'ì·¨ì—…', 'ì¼ìë¦¬', 'ì±„ìš©']):
                title = f"ğŸ¯ {title}"
            
            logger.info(f"\n{i}. {title}")
            logger.info(f"   ì €ì: {paper.get('author', 'N/A')}")
            logger.info(f"   ê¸°ê´€: {paper.get('publisher', 'N/A')}")
            logger.info(f"   ì—°ë„: {paper.get('pub_year', 'N/A')}")
        
        # DataFrame ìƒì„±
        df = pd.DataFrame(results)
        
        # Excel ë‚´ë³´ë‚´ê¸°
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"riss_improved_{search_term}_{timestamp}.xlsx"
        logger.info(f"\nğŸ’¾ Excel íŒŒì¼ ì €ì¥ ì¤‘: {filename}")
        analyzer.export_results(results, filename, search_term)
        
        logger.info(f"\nğŸ‰ ì™„ë£Œ!")
        
    else:
        logger.warning("âŒ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()